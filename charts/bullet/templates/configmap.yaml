apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "bullet.fullname" . }}
  labels:
    {{- include "bullet.labels" . | nindent 4 }}
data:
  bullet_ui_env_settings.json: |-
    {
      "default": {
        "queryHost": {{ .Values.bullet.ui.config.queryHost | quote }},
        "queryNamespace": "api/bullet/queries",
        "queryPath": "ws-query",
        "validationPath": "validate-query",
        "queryStompRequestChannel": "/server/request",
        "queryStompResponseChannel": "/client/response",
        "schemaHost": {{ .Values.bullet.ui.config.schemaHost | quote }},
        "schemaNamespace": "api/bullet",
        "helpLinks": [
          {
            "name": "Tutorials",
            "link": "https://bullet-db.github.io/ui/usage"
          }
        ],
        "bugLink": "https://github.com/bullet-db/bullet-ui/issues",
        "modelVersion": 4,
        "migrations": {
          "deletions": "query"
        },
        "defaultValues": {
          "aggregationMaxSize": 1024,
          "rawMaxSize": 500,
          "durationMaxSecs": 9007199254740,
          "distributionNumberOfPoints": 11,
          "distributionQuantilePoints": "0, 0.25, 0.5, 0.75, 0.9, 1",
          "distributionQuantileStart": 0,
          "distributionQuantileEnd": 1,
          "distributionQuantileIncrement": 0.1,
          "windowEmitFrequencyMinSecs": 1,
          "everyForRecordBasedWindow": 1,
          "everyForTimeBasedWindow": 2000,
          "sketches": {
            "countDistinctMaxEntries": 16384,
            "groupByMaxEntries": 512,
            "distributionMaxEntries": 1024,
            "distributionMaxNumberOfPoints": 200,
            "topKMaxEntries": 1024,
            "topKErrorType": "No False Negatives"
          },
          "metadataKeyMapping": {
            "querySection": "Query",
            "windowSection": "Window",
            "sketchSection": "Sketch",
            "theta": "Theta",
            "uniquesEstimate": "Uniques Estimate",
            "queryCreationTime": "Receive Time",
            "queryTerminationTime": "Finish Time",
            "estimatedResult": "Was Estimated",
            "standardDeviations": "Standard Deviations",
            "normalizedRankError": "Normalized Rank Error",
            "maximumCountError": "Maximum Count Error",
            "itemsSeen": "Items Seen",
            "minimumValue": "Minimum Value",
            "maximumValue": "Maximum Value",
            "windowNumber": "Number",
            "windowSize": "Size",
            "windowEmitTime": "Emit Time",
            "expectedEmitTime": "Expected Emit Time"
          }
        }
      }
    }
  bullet_webservice_pubsub_config.yaml: |-
    bullet.pubsub.context.name: "QUERY_SUBMISSION"
    bullet.pubsub.class.name: "com.yahoo.bullet.kafka.KafkaPubSub"
    bullet.pubsub.kafka.bootstrap.servers: {{ printf "bullet-pubsub:%g" .Values.bullet.webservice.pubsub.port | quote}}
    bullet.pubsub.kafka.request.topic.name: {{ .Values.bullet.webservice.config.bullet.pubsub.kafka.request.topic.name | quote }}
    bullet.pubsub.kafka.response.topic.name: {{ .Values.bullet.webservice.config.bullet.pubsub.kafka.response.topic.name | quote }}
    bullet.pubsub.message.serde.class.name: "com.yahoo.bullet.pubsub.IdentityPubSubMessageSerDe"
  bullet_webservice_query_config.yaml: |-
    bullet.query.aggregation.max.size: {{ .Values.bullet.webservice.config.bullet.query.aggregation.max.size | int }}
  bullet_webservice_schema.json: |-
    [
        {
            "name": "probability",
            "type": "DOUBLE",
            "description": "Generated from Random#nextDouble"
        },
        {
            "name": "gaussian",
            "type": "DOUBLE",
            "description": "Generated from Random#nextGaussian"
        },
        {
            "name": "uuid",
            "type": "STRING",
            "description": "A UUID string generated from UUID#randomUUID"
        },
        {
            "name": "tuple_number",
            "type": "LONG",
            "description": "A numeric id for the tuple generated in a monotonically increasing fashion in this period"
        },
        {
            "name": "duration",
            "type": "LONG",
            "description": "A random number ranging from 0 to 10050 with a tendency to have a high frequency on lower values"
        },
        {
            "name": "type",
            "type": "STRING",
            "description": "A random string chosen from: foo, bar, baz, qux, quux, norf"
        },
        {
            "name": "subtypes",
            "type": "STRING_MAP",
            "description": "Contains two keys whose values are randomly chosen from: foo, bar, baz, qux, quux, norf",
            "subFields": [
                {"name": "field_A", "description": "Value randomly chosen from: foo, bar, baz, qux, quux, norf"},
                {"name": "field_B", "description": "Value randomly chosen from: foo, bar, baz, qux, quux, norf"}
            ]
        },
        {
            "name": "tags",
            "type": "BOOLEAN_MAP",
            "description": "Contains four keys which are four fragments of the uuid. The values are randomly generated boolean values from Random#nextBoolean"
        },
        {
            "name": "stats",
            "type": "LONG_MAP",
            "description": "This map contains some numeric information such as the current number of periods etc.",
            "subFields": [
                {"name": "period_count", "description": "The period in which this record was generated"},
                {"name": "record_number", "description": "A monotonically increasing id for the record. There may be gaps in the id but if the data generation has kept up with your maximum tuples per period, this is the nth tuple generated"},
                {"name": "timestamp", "description": "The ms time when this record was generated"},
                {"name": "nano_time", "description": "The ns time when this record was generated"}
            ]
        },
        {
            "name": "classifiers",
            "type": "STRING_MAP_LIST",
            "description": "This contains two maps, each with: field_A and field_B whose values are randomly chosen from: foo, bar, baz, qux, quux, norf"
        }
    ]
  bullet_spark_settings.yaml: |-
    ########################################################################################################################
    ################################################  Bullet Spark config ##################################################
    ########################################################################################################################
    # This is the name of the concrete implementation of Data Producer to use.
    bullet.spark.data.producer.class.name: {{ .Values.bullet.sparkBackend.config.bullet.spark.data.producer.class.name | quote }}

    # If true, enables the Bullet DSL data producer which can be configured to read from a custom data source. If enabled,
    # the DSL data producer is used instead of the producer provided above. (See https://github.com/bullet-db/bullet-dsl)
    bullet.spark.dsl.data.producer.enable: {{ .Values.bullet.sparkBackend.config.bullet.spark.dsl.data.producer.enable }}

    # If true, enables the deserializer between the Bullet DSL connector and converter components. Otherwise, this step is
    # skipped.
    bullet.spark.dsl.deserializer.enable: {{ .Values.bullet.sparkBackend.config.bullet.spark.dsl.deserializer.enable }}

    # This is the batch interval of your Spark Streaming job. Find out more at
    # https://spark.apache.org/docs/latest/streaming-programming-guide.html#setting-the-right-batch-interval.
    bullet.spark.batch.duration.ms: {{ .Values.bullet.sparkBackend.config.bullet.spark.batch.duration.ms }}

    # This is the size of the buffer for accumulating queries in the Query Receiver before emitting to Spark.
    bullet.spark.receiver.query.block.size: {{ .Values.bullet.sparkBackend.config.bullet.spark.receiver.query.block.size }}

    # This is the maximum number of partitions that will be created by the Query Receiver.
    bullet.spark.receiver.query.coalesce.partitions: {{ .Values.bullet.sparkBackend.config.bullet.spark.receiver.query.coalesce.partitions }}

    # This is the number of Data Producers.
    bullet.spark.data.producer.parallelism: {{ .Values.bullet.sparkBackend.config.bullet.spark.data.producer.parallelism }}

    # This is the checkpoint directory. If you are running your Spark on a cluster, the directory must be an HDFS path.
    bullet.spark.checkpoint.dir: {{ printf "hdfs://%s-hdfs:%g%s" .Release.Name .Values.bullet.sparkBackend.hdfs.port .Values.bullet.sparkBackend.config.bullet.spark.checkpoint.dir | quote}}

    # If true, Bullet Spark recovers context from checkpoint files when restarting.
    # Otherwise Bullet Spark creates a new context.
    bullet.spark.recover.from.checkpoint.enable: {{ .Values.bullet.sparkBackend.config.bullet.spark.recover.from.checkpoint.enable }}

    # This is the Spark application name.
    bullet.spark.app.name: {{ .Values.bullet.sparkBackend.config.bullet.spark.app.name | quote }}

    # If true, Bullet Spark collects metrics which can be accessed via the Spark REST API (/metrics/json).
    bullet.spark.metrics.enabled: {{ .Values.bullet.sparkBackend.config.bullet.spark.metrics.enabled }}

    # If true, enables parallel processing of queries in each partition of the Filter Streaming job, This is particularly
    # useful when using Producers that are Direct (e.g. DirectKafkaProducer) and you would like to avoid repartitioning
    # the data and instead choose to parallelize within each partition (fixed by the producer) instead.
    # It speeds up the processing within those partitions by partitioning queries to multiple threads to do the filtering
    # operation concurrently.
    bullet.spark.filter.partition.parallel.mode.enabled: {{ .Values.bullet.sparkBackend.config.bullet.spark.filter.partition.parallel.mode.enabled }}

    # This is the thread pool size to use when bullet.spark.filter.partition.parallel.mode.enabled is true.
    bullet.spark.filter.partition.parallel.mode.parallelism: {{ .Values.bullet.sparkBackend.config.bullet.spark.filter.partition.parallel.mode.parallelism }}

    # This is the minimum number of queries at which the parallel partition filtering is applied. Since there are fixed
    # costs to manage a thread pool, they are only created once the number of queries exceeds this threshold.
    # It is only used when bullet.spark.filter.partition.parallel.mode.enabled is true.
    bullet.spark.filter.partition.parallel.mode.min.query.threshold: {{ .Values.bullet.sparkBackend.config.bullet.spark.filter.partition.parallel.mode.min.query.threshold }}

    # The following 2 settings are used to set the checkpoint intervals independently for each stateful transformation.
    # Checkpoint interval = Spark duration * checkpoint duration multiplier
    # Use this to control the frequency of checkpointing operation. If this is set too high, there might be too much
    # data to checkpoint (RDD lineage graph).
    bullet.spark.query.union.checkpoint.duration.multiplier: {{ .Values.bullet.sparkBackend.config.bullet.spark.query.union.checkpoint.duration.multiplier }}
    bullet.spark.join.checkpoint.duration.multiplier: {{ .Values.bullet.sparkBackend.config.bullet.spark.join.checkpoint.duration.multiplier }}

    # The feedback publisher switches your PubSub into QUERY_SUBMISSION mode to loop back metadata messages to query
    # receiver. If you need to change settings for your publisher in this mode that is different from the settings
    # used in the result publisher, override them here. This setting needs to be a Map if provided.
    # The example below pretends that your PubSub settings start with bullet.pubsub.custom. You will provide yours.
    # Example:
    #
    # bullet.spark.loop.pubsub.overrides:
    #   bullet.pubsub.custom.publisher.setting: 1
    #   bullet.pubsub.custom.nested.publisher.setting:
    #     foo: bar
    #     bar: baz
    bullet.spark.loop.pubsub.overrides: {}

    ########################################################################################################################
    ################################################ Spark Streaming config ################################################
    ########################################################################################################################
    # The following settings are passed to Spark directly. You can add more settings here.
    # Find out more information about configuring a Spark job at https://spark.apache.org/docs/latest/configuration.html.
    # Add configuration that change infrequently here and submit more variable settings while submitting the job on the
    # command line.
    spark.serializer: {{ .Values.bullet.sparkBackend.config.spark.serializer | quote }}
    spark.closure.serializer: {{ .Values.bullet.sparkBackend.config.spark.closure.serializer | quote }}
    spark.streaming.stopGracefullyOnShutdown: {{ .Values.bullet.sparkBackend.config.spark.streaming.stopGracefullyOnShutdown | quote }}
    spark.streaming.receiver.writeAheadLog.enable: {{ .Values.bullet.sparkBackend.config.spark.streaming.receiver.writeAheadLog.enable | quote }}
    spark.streaming.driver.writeAheadLog.allowBatching: {{ .Values.bullet.sparkBackend.config.spark.streaming.driver.writeAheadLog.allowBatching | quote }}

    ########################################################################################################################
    ################################################ Query PubSub config ###################################################
    ########################################################################################################################
    # This is the type of PubSub context to use for result publisher.
    # The feedback publisher uses QUERY_SUBMISSION since it submits messages.
    bullet.pubsub.context.name: {{ .Values.bullet.sparkBackend.config.bullet.pubsub.context.name | quote }}
    # This is the name of the concrete implementation of PubSub to use.
    # By default, it is the bulletin REST in-memory PubSub.
    bullet.pubsub.class.name: {{ .Values.bullet.sparkBackend.config.bullet.pubsub.class.name | quote }}
    # Add settings specific to your PubSub.
    bullet.pubsub.kafka.bootstrap.servers: {{ .Values.bullet.sparkBackend.config.bullet.pubsub.kafka.bootstrap.servers | quote }}
    bullet.pubsub.kafka.request.topic.name: {{ .Values.bullet.sparkBackend.config.bullet.pubsub.kafka.request.topic.name | quote }}
    bullet.pubsub.kafka.response.topic.name: {{ .Values.bullet.sparkBackend.config.bullet.pubsub.kafka.response.topic.name | quote }}
    bullet.pubsub.message.serde.class.name: "com.yahoo.bullet.pubsub.IdentityPubSubMessageSerDe"

    ########################################################################################################################
    ################################################ Bullet Core config ####################################################
    ########################################################################################################################
    ## You can also configure the core Bullet settings here. For documentation and defaults for those settings, refer to:
    ## https://github.com/bullet-db/bullet-core/blob/master/src/main/resources/bullet_defaults.yaml
    ########################################################################################################################
    ########################################################################################################################
    bullet.query.aggregation.raw.max.size: {{ .Values.bullet.sparkBackend.config.bullet.query.aggregation.raw.max.size }}
    # This setting is enforced in the API at this time
    # bullet.query.aggregation.max.size: 1024
    bullet.query.aggregation.count.distinct.sketch.entries: {{ .Values.bullet.sparkBackend.config.bullet.query.aggregation.count.distinct.sketch.entries }}
    bullet.query.aggregation.group.sketch.entries: {{ .Values.bullet.sparkBackend.config.bullet.query.aggregation.group.sketch.entries }}
    bullet.query.aggregation.distribution.sketch.entries: {{ .Values.bullet.sparkBackend.config.bullet.query.aggregation.distribution.sketch.entries }}
    bullet.query.aggregation.distribution.max.points: {{ .Values.bullet.sparkBackend.config.bullet.query.aggregation.distribution.max.points }}
    bullet.query.aggregation.distribution.generated.points.rounding: {{ .Values.bullet.sparkBackend.config.bullet.query.aggregation.distribution.generated.points.rounding }}
    bullet.query.aggregation.top.k.sketch.entries: {{ .Values.bullet.sparkBackend.config.bullet.query.aggregation.top.k.sketch.entries }}
    bullet.query.aggregation.top.k.sketch.error.type: {{ .Values.bullet.sparkBackend.config.bullet.query.aggregation.top.k.sketch.error.type | quote }}
    bullet.result.metadata.enable: {{ .Values.bullet.sparkBackend.config.bullet.result.metadata.enable }}
    # Factory class to get new BulletRecords.
    bullet.record.provider.class.name: {{ .Values.bullet.sparkBackend.config.bullet.record.provider.class.name | quote }}
    {{- if .Values.bullet.sparkBackend.config.bullet.spark.dsl.data.producer.enable }}
    ########################################################################################################################
    ################################################ Bullet DSL config #####################################################
    ########################################################################################################################
    ## For documentation and defaults for those settings, refer to:
    ## https://github.com/bullet-db/bullet-dsl/blob/master/src/main/resources/bullet_dsl_defaults.yaml
    ########################################################################################################################
    ########################################################################################################################
    # The classpath to the BulletConnector to use
    bullet.dsl.connector.class.name: {{ .Values.bullet.sparkBackend.config.bullet.dsl.connector.class.name | quote }}
    # The read timeout duration in ms (defaults to 0)
    bullet.dsl.connector.read.timeout.ms: {{ .Values.bullet.sparkBackend.config.bullet.dsl.connector.read.timeout.ms }}
    # Whether or not to asynchronously commit messages (defaults to true)
    bullet.dsl.connector.async.commit.enable: {{ .Values.bullet.sparkBackend.config.bullet.dsl.connector.async.commit.enable }}

    ###### KafkaConnector properties

    # The list of Kafka topics to subscribe to (required)
    bullet.dsl.connector.kafka.topics:
    {{- range $topic := .Values.bullet.sparkBackend.config.bullet.dsl.connector.kafka.topics }}
    - {{ $topic | quote }}
    {{- end }}
    # Whether or not the KafkaConsumer should seek to the end of its subscribed topics at initialization (defaults to false)
    bullet.dsl.connector.kafka.start.at.end.enable: {{ .Values.bullet.sparkBackend.config.bullet.dsl.connector.kafka.start.at.end.enable }}

    # Kafka properties (prefixed by "bullet.dsl.connector.kafka.") are passed to KafkaConsumer during construction with the
    # prefix removed. The properties below are required.
    # Properties found at: https://kafka.apache.org/20/javadoc/org/apache/kafka/clients/consumer/ConsumerConfig.html
    bullet.dsl.connector.kafka.bootstrap.servers: {{ .Values.bullet.sparkBackend.config.bullet.dsl.connector.kafka.bootstrap.servers | quote }}
    bullet.dsl.connector.kafka.group.id: {{ .Values.bullet.sparkBackend.config.bullet.dsl.connector.kafka.group.id | quote }}
    bullet.dsl.connector.kafka.key.deserializer: {{ .Values.bullet.sparkBackend.config.bullet.dsl.connector.kafka.key.deserializer | quote }}
    bullet.dsl.connector.kafka.value.deserializer: {{ .Values.bullet.sparkBackend.config.bullet.dsl.connector.kafka.value.deserializer | quote }}

    ###### BulletRecordConverter properties

    # The classpath to the BulletRecordConverter to use
    bullet.dsl.converter.class.name: {{ .Values.bullet.sparkBackend.config.bullet.dsl.converter.class.name | quote }}
    # The path to the schema file to use
    bullet.dsl.converter.schema.file: {{ .Values.bullet.sparkBackend.config.bullet.dsl.converter.schema.file | quote }}
    # Should type checking be performed for fields with type in the schema. It is useful to make sure that the types in
    # your source records match your expectations. You can set this to false when you are sure of your schema. This is
    # ignored if you do not provide a schema.
    bullet.dsl.converter.schema.type.check.enable: {{ .Values.bullet.sparkBackend.config.bullet.dsl.converter.schema.type.check.enable }}

    ###### BulletDeserializer properties

    # The classpath to the BulletDeserializer to use
    bullet.dsl.deserializer.class.name: {{ .Values.bullet.sparkBackend.config.bullet.dsl.deserializer.class.name | quote }}
    {{- end }}
  dsl_schema.json: |-
{{ .Values.bullet.sparkBackend.dsl.schema | indent 4 }}
